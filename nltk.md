## **I. Import**
```python
import nltk
```

## **II. Tokenization**
```python
# Break a string up into words (punctuation will not be part of the word)
from nltk.tokenize import word_tokenize
word_tokenize("A long piece. of string. with some, punctuation")

# NOTE: Better way is to use spacy
```
